---
title: Lasso
layout: default
output: bookdown::html_chapter
---

# Chapter 11, Lasso

```{r setup, echo=FALSE}
knitr::opts_chunk$set(fig.path="Ch11-figures/")
```

This goal of this chapter is to create an interactive data
visualization that explains the
[Lasso](https://en.wikipedia.org/wiki/Lasso_%28statistics%29), a
machine learning model for regularized linear regression.

## Fix one train/validation split

We begin by loading the prostate cancer data set.

```{r}
data(prostate, package="ElemStatLearn")
head(prostate)
```

We construct a train inputs `x` and outputs `y` using the code below.

```{r}
input.cols <- c(
  "lcavol", "lweight", "age", "lbph", "svi", "lcp", "gleason", 
  "pgg45")
is.validation <- prostate$train == FALSE
train.df <- prostate[!is.validation, ]
x <- as.matrix(train.df[input.cols])
head(x)
y <- train.df$lpsa
head(y)
```

Below we fit the full path of lasso solutions using the `lars` package.

```{r}
library(lars)
fit <- lars(x,y,type="lasso")
fit$lambda
```

The path of `lambda` values are not evenly spaced.

```{r}
pred.nox <- predict(fit, type="coef")
beta <- scale(pred.nox$coefficients, FALSE, 1/fit$normx)
arclength <- rowSums(abs(beta))
path.list <- list()
library(data.table)
for(variable in colnames(beta)){
  standardized.coef <- beta[, variable]
  path.list[[variable]] <- data.table(
    step=seq_along(standardized.coef),
    lambda=c(fit$lambda, 0),
    variable,
    standardized.coef,
    fraction=pred.nox$fraction,
    arclength)
}
path <- do.call(rbind, path.list)
variable.colors <- c(
  "#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#FFFF33", 
  "#A65628", "#F781BF", "#999999")
gg.lambda <- ggplot()+
  theme_bw()+
  theme(panel.margin=grid::unit(0, "lines"))+
  scale_color_manual(values=variable.colors)+
  geom_line(aes(lambda, standardized.coef, color=variable, group=variable),
            data=path)+
  ggtitle("LASSO path for prostate cancer data calculated using the LARS")
gg.lambda
```

The plot above shows the entire lasso path, the optimal weights in the
L1-regularized least squares regression problem, for every
regularization parameter lambda. The path begins at the least squares
solution, lambda=0 on the left. It ends at the completely regularized
intercept-only model on the right. To see the equivalence with the
ordinary least squares solution, we add dots in the plot below.

```{r}
x.scaled <- with(fit, scale(x, meanx, normx))
lfit <- lm.fit(x.scaled, y)
lpoints <- data.table(
  variable=colnames(x),
  standardized.coef=lfit$coefficients,
  arclength=sum(abs(lfit$coefficients)))
gg.lambda+
  geom_point(aes(0, standardized.coef, color=variable),
             data=lpoints)
```

In the next plot below, we show the path as a function of L1 norm
(arclength), with some more points on an evenly spaced grid that we
will use later for animation.

```{r}
fraction <- sort(unique(c(
  seq(0, 1, l=21))))
pred.list <- predict(
  fit, prostate[input.cols],
  type="coef", mode="fraction", s=fraction)
coef.grid.list <- list()
coef.grid.mat <- scale(pred.list$coefficients, FALSE, 1/fit$normx)
for(fraction.i in seq_along(fraction)){
  standardized.coef <- coef.grid.mat[fraction.i,]
  coef.grid.list[[fraction.i]] <- data.table(
    fraction=fraction[[fraction.i]],
    variable=colnames(x),
    standardized.coef,
    arclength=sum(abs(standardized.coef)))
}
coef.grid <- do.call(rbind, coef.grid.list)
ggplot()+
  ggtitle("LASSO path for prostate cancer data calculated using the LARS")+
  theme_bw()+
  theme(panel.margin=grid::unit(0, "lines"))+
  scale_color_manual(values=variable.colors)+
  geom_line(aes(arclength, standardized.coef, color=variable, group=variable),
            data=path)+
  geom_point(aes(arclength, standardized.coef, color=variable),
             data=lpoints)+
  geom_point(aes(arclength, standardized.coef, color=variable),
             shape=21,
             fill=NA,
             size=3,
             data=coef.grid)
```

```{r}
residual.mat <- pred.list$fit - prostate$lpsa
squares.mat <- residual.mat * residual.mat
mean.error.list <- list()
for(set in c("train", "validation")){
  val <- if(set=="validation")TRUE else FALSE
  is.set <- is.validation == val
  mse <- colMeans(squares.mat[is.set, ])
  mean.error.list[[paste(set)]] <- data.table(
    set, mse, fraction, what="mean squared error")
}
mean.error <- do.call(rbind, mean.error.list)
```

and plot its coefficients.


```{r}
for(fraction.i in seq_along(fraction)){
  f <- fraction[fraction.i]
  obs.error.list[[paste(fraction.i)]] <- 
    data.frame(
      observation.i=1:nrow(residual.mat),
      set=ifelse(is.validation, "validation", "train"),
      fraction.i,
      fraction.fac=factor(f, fraction),
      fraction=f,
      residual=residual.mat[, fraction.i],
      predicted=pred.list$fit[, fraction.i],
      true=prostate$lpsa)
}

selected.list[[paste(validation.fold)]] <-
  subset(error.df, mse==min(mse))
zeros.list[[paste(validation.fold)]] <-
  data.frame(validation.fold,
             zeros=rowSums(beta==0),
             fraction=pred.nox$fraction)

```


## Several train sets

## Chapter summary and exercises
