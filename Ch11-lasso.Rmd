---
title: Lasso
layout: default
output: bookdown::html_chapter
---

# Chapter 11, Lasso

```{r setup, echo=FALSE}
knitr::opts_chunk$set(fig.path="Ch11-figures/")
```

This goal of this chapter is to create an interactive data
visualization that explains the
[Lasso](https://en.wikipedia.org/wiki/Lasso_%28statistics%29), a
machine learning model for regularized linear regression.

## Fix one train/validation split

We begin by loading the prostate cancer data set.

```{r}
data(prostate, package="ElemStatLearn")
head(prostate)
```

We construct a train inputs `x` and outputs `y` using the code below.

```{r}
input.cols <- c(
  "lcavol", "lweight", "age", "lbph", "svi", "lcp", "gleason", 
  "pgg45")
is.validation <- prostate$train == FALSE
train.df <- prostate[!is.validation, ]
x <- as.matrix(train.df[input.cols])
head(x)
y <- train.df$lpsa
head(y)
```

Below we fit the full path of lasso solutions using the `lars` package.

```{r}
library(lars)
fit <- lars(x,y,type="lasso")
fit$lambda
```

The path of `lambda` values are not evenly spaced.

```{r}
pred.nox <- predict(fit, type="coef")
beta <- scale(pred.nox$coefficients, FALSE, 1/fit$normx)
arclength <- rowSums(abs(beta))
path.list <- list()
library(data.table)
for(variable in colnames(beta)){
  standardized.coef <- beta[, variable]
  path.list[[variable]] <- data.table(
    step=seq_along(standardized.coef),
    lambda=c(fit$lambda, 0),
    variable,
    standardized.coef,
    fraction=pred.nox$fraction,
    arclength)
}
path <- do.call(rbind, path.list)

variable.colors <- c(
  "#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#FFFF33", 
  "#A65628", "#F781BF", "#999999")
ggplot()+
  theme_bw()+
  theme(panel.margin=grid::unit(0, "lines"))+
  scale_color_manual(values=variable.colors)+
  geom_line(aes(lambda, standardized.coef, color=variable, group=variable),
            data=path)+
  ggtitle("LASSO path for prostate cancer data calculated using the LARS")
```

```{r}
fraction <- sort(unique(c(
  seq(0, 1, l=21))))
pred.list <- predict(
  fit, prostate[input.cols],
  mode="fraction", s=fraction)
```

```{r}
residual.mat <- pred.list$fit - y
squares.mat <- residual.mat * residual.mat
for(set in c("train", "validation")){
  val <- if(set=="validation")TRUE else FALSE
  is.set <- is.validation == val
  mse <- colMeans(squares.mat[is.set, ])
  mean.error.list[[paste(set)]] <- error.df <- data.frame(
    set, mse, fraction, what="mean squared error")
}

```

and plot its coefficients.


```{r}
for(fraction.i in seq_along(fraction)){
  f <- fraction[fraction.i]
  obs.error.list[[paste(fraction.i)]] <- 
    data.frame(
      observation.i=1:nrow(residual.mat),
      set=ifelse(is.validation, "validation", "train"),
      fraction.i,
      fraction.fac=factor(f, fraction),
      fraction=f,
      residual=residual.mat[, fraction.i],
      predicted=pred.list$fit[, fraction.i],
      true=prostate$lpsa)
}

selected.list[[paste(validation.fold)]] <-
  subset(error.df, mse==min(mse))
zeros.list[[paste(validation.fold)]] <-
  data.frame(validation.fold,
             zeros=rowSums(beta==0),
             fraction=pred.nox$fraction)

```


## Several train sets

## Chapter summary and exercises
