---
title: Lasso
layout: default
output: bookdown::html_chapter
---

# Chapter 11, Lasso

```{r setup, echo=FALSE}
knitr::opts_chunk$set(fig.path="Ch11-figures/")
```

This goal of this chapter is to create an interactive data
visualization that explains the
[Lasso](https://en.wikipedia.org/wiki/Lasso_%28statistics%29), a
machine learning model for regularized linear regression.

## Fix one train/validation split

We begin by loading the prostate cancer data set.

```{r}
data(prostate, package="ElemStatLearn")
head(prostate)
```

We construct a train inputs `x` and outputs `y` using the code below.

```{r}
input.cols <- c(
  "lcavol", "lweight", "age", "lbph", "svi", "lcp", "gleason", 
  "pgg45")
is.validation <- prostate$train == FALSE
train.df <- prostate[!is.validation, ]
x <- as.matrix(train.df[input.cols])
head(x)
y <- train.df$lpsa
head(y)
```

Below we fit the full path of lasso solutions using the `lars` package.

```{r}
library(lars)
fit <- lars(x,y,type="lasso")
fit$lambda
```

The path of `lambda` values are not evenly spaced.

```{r}
pred.nox <- predict(fit, type="coef")
beta <- scale(pred.nox$coefficients, FALSE, 1/fit$normx)
arclength <- rowSums(abs(beta))
path.list <- list()
library(data.table)
for(variable in colnames(beta)){
  standardized.coef <- beta[, variable]
  path.list[[variable]] <- data.table(
    step=seq_along(standardized.coef),
    lambda=c(fit$lambda, 0),
    variable,
    standardized.coef,
    fraction=pred.nox$fraction,
    arclength)
}
path <- do.call(rbind, path.list)
variable.colors <- c(
  "#E41A1C", "#377EB8", "#4DAF4A", "#984EA3", "#FF7F00", "#FFFF33", 
  "#A65628", "#F781BF", "#999999")
gg.lambda <- ggplot()+
  theme_bw()+
  theme(panel.margin=grid::unit(0, "lines"))+
  scale_color_manual(values=variable.colors)+
  geom_line(aes(lambda, standardized.coef, color=variable, group=variable),
            data=path)+
  ggtitle("LASSO path for prostate cancer data calculated using the LARS")
gg.lambda
```

The plot above shows the entire lasso path, the optimal weights in the
L1-regularized least squares regression problem, for every
regularization parameter lambda. The path begins at the least squares
solution, lambda=0 on the left. It ends at the completely regularized
intercept-only model on the right. To see the equivalence with the
ordinary least squares solution, we add dots in the plot below.

```{r}
x.scaled <- with(fit, scale(x, meanx, normx))
lfit <- lm.fit(x.scaled, y)
lpoints <- data.table(
  variable=colnames(x),
  standardized.coef=lfit$coefficients,
  arclength=sum(abs(lfit$coefficients)))
gg.lambda+
  geom_point(aes(0, standardized.coef, color=variable),
             data=lpoints)
```

In the next plot below, we show the path as a function of L1 norm
(arclength), with some more points on an evenly spaced grid that we
will use later for animation.

```{r}
fraction <- sort(unique(c(
  seq(0, 1, l=21))))
pred.fraction <- predict(
  fit, prostate[input.cols],
  type="coef", mode="fraction", s=fraction)
coef.grid.list <- list()
coef.grid.mat <- scale(pred.fraction$coefficients, FALSE, 1/fit$normx)
for(fraction.i in seq_along(fraction)){
  standardized.coef <- coef.grid.mat[fraction.i,]
  coef.grid.list[[fraction.i]] <- data.table(
    fraction=fraction[[fraction.i]],
    variable=colnames(x),
    standardized.coef,
    arclength=sum(abs(standardized.coef)))
}
coef.grid <- do.call(rbind, coef.grid.list)
ggplot()+
  ggtitle("LASSO path for prostate cancer data calculated using the LARS")+
  theme_bw()+
  theme(panel.margin=grid::unit(0, "lines"))+
  scale_color_manual(values=variable.colors)+
  geom_line(aes(arclength, standardized.coef, color=variable, group=variable),
            data=path)+
  geom_point(aes(arclength, standardized.coef, color=variable),
             data=lpoints)+
  geom_point(aes(arclength, standardized.coef, color=variable),
             shape=21,
             fill=NA,
             size=3,
             data=coef.grid)
```

The plot above shows that the weights at the grid points are
consistent with the lines that represent the entire path of
solutions. The LARS algorithm quickly provides Lasso solutions for as
many grid points as you like. More precisely, since the LARS only
computes the change-points in the piecewise linear path, its time
complexity only depends on the number of change-points (not the number
of grid points).

The plot below combines the lasso weight path with the train/test
error plot.

```{r}
pred.list <- predict(
  fit, prostate[input.cols],
  mode="fraction", s=fraction)
residual.mat <- pred.list$fit - prostate$lpsa
squares.mat <- residual.mat * residual.mat
mean.error.list <- list()
for(set in c("train", "validation")){
  val <- if(set=="validation")TRUE else FALSE
  is.set <- is.validation == val
  mse <- colMeans(squares.mat[is.set, ])
  mean.error.list[[paste(set)]] <- data.table(
    set, mse, fraction,
    arclength=rowSums(abs(coef.grid.mat)))
}
mean.error <- do.call(rbind, mean.error.list)
rect.width <- diff(mean.error$arclength[1:2])/2
library(animint)
addY <- function(dt, y){
  data.table(dt, y.var=factor(y, c("error", "weights")))
}
ggplot()+
  ggtitle("LASSO path for prostate cancer data calculated using the LARS")+
  theme_bw()+
  theme(panel.margin=grid::unit(0, "lines"))+
  facet_grid(y.var ~ ., scales="free")+
  ylab("")+
  scale_color_manual(values=variable.colors)+
  geom_line(aes(arclength, standardized.coef, color=variable, group=variable),
            data=addY(path, "weights"))+
  geom_line(aes(arclength, mse, linetype=set),
            data=addY(mean.error, "error"))+
  geom_tallrect(aes(
    xmin=arclength-rect.width,
    xmax=arclength+rect.width,
    clickSelects=arclength),
    alpha=0.5,
    data=coef.grid)
```

Finally, we add a plot of residuals versus actual values.

```{r}
lasso.res.list <- list()
for(fraction.i in seq_along(fraction)){
  lasso.res.list[[fraction.i]] <- data.table(
    observation.i=1:nrow(prostate),
    fraction=fraction[[fraction.i]],
    residual=residual.mat[, fraction.i],
    response=prostate$lpsa,
    arclength=sum(abs(coef.grid.mat[fraction.i,])),
    set=ifelse(prostate$train, "train","validation"))
}
lasso.res <- do.call(rbind, lasso.res.list)
ggplot()+
  geom_point(aes(response, residual, fill=set, showSelected=arclength),
             shape=21,
             data=lasso.res)
```

TODO combine in a single animint.

## Several train sets

## Chapter summary and exercises
