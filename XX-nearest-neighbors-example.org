Chapter XX, visualizing the nearest neighbors classifier

In this chapter we will explore several data
visualizations of the K-Nearest Neighbors (KNN) classifier.

* Original static figure

We start by reproducing a static version of Figure 13.4 from Elements
of Statistical Learning by Hastie et al. That Figure consists of two
plots (TODO SKETCH):
- mis-classification error curves, as a function of the number of
  neighbors:
  - geom_line and geom_point for the error curves.
  - geom_linerange for error bars of the validation error curve.
  - geom_hline for the Bayes error.
  - x = neighbors.
  - y = percent error.
  - color = error type.
- data and decision boundaries in the two-dimensional input feature
  space.
  - geom_point for the data points.
  - geom_point for the classification predictions on the grid in the
    background.
  - geom_path for the decision boundaries.
  - geom_text for the train/test/Bayes error rates.

** Plot of mis-classification error curves

We begin by loading the data set.

#+BEGIN_SRC R
  library(ElemStatLearn)
  data(mixture.example)
  str(mixture.example)
#+END_SRC

We will use the following components of this data set:
- x, the input matrix of the training data set (200 observations x 2
  numeric features). 
- y, the output vector of the training data set (200 class labels,
  either 0 or 1).
- xnew, the grid of points in the input space where we will show the
  classifier predictions (6831 grid points x 2 numeric features).
- prob, the true probability of class 1 at each of the grid points
  (6831 numeric values between 0 and 1).
- px1, the grid of points for the first input feature (69 numeric
  values between -2.6 and 4.2). These will be used to compute the
  Bayes decision boundary using the contourLines function.
- px2, the grid of points for the second input feature (99 numeric
  values between -2 and 2.9).
- means, the 20 centers of the normal distributions in the simulation
  model (20 centers x 2 input features).

First, we create a test set, following the example code from
help(mixture.example).

#+BEGIN_SRC R
  library(MASS)
  library(data.table)
  set.seed(123)
  centers <- c(sample(1:10, 5000, replace=TRUE), 
               sample(11:20, 5000, replace=TRUE))
  mix.test <- mvrnorm(10000, c(0,0), 0.2*diag(2))
  test.points <- data.table(
    mix.test + mixture.example$means[centers,],
    label=factor(c(rep(0, 5000), rep(1, 5000))))
  test.points
#+END_SRC  

We then create a data table which includes all test points and grid
points, which we will use in the test argument to the knn function.

#+BEGIN_SRC R
  pred.grid <- data.table(mixture.example$xnew, label=NA)
  input.cols <- c("V1", "V2")
  names(pred.grid)[1:2] <- input.cols
  test.and.grid <- rbind(
    data.table(test.points, set="test"),
    data.table(pred.grid, set="grid"))
  test.and.grid$fold <- NA
  test.and.grid
#+END_SRC  

We randomly assign each observation of the training data set to one of
ten folds.

#+BEGIN_SRC R
  n.folds <- 10
  set.seed(2)
  mixture <- with(mixture.example, data.table(x, label=factor(y)))
  mixture$fold <- sample(rep(1:n.folds, l=nrow(mixture)))
  mixture
#+END_SRC

We define the following OneFold function, which divides the 200
observations into one train and one validation set. It then computes
the predicted probability of the K-Nearest-Neighbors classifier for
each of the data points in all sets (train, validation, test, and
grid).

#+BEGIN_SRC R
  OneFold <- function(validation.fold){
    require(class)
    set <- ifelse(mixture$fold == validation.fold, "validation", "train")
    fold.data <- rbind(test.and.grid, data.table(mixture, set))
    fold.data$data.i <- 1:nrow(fold.data)
    only.train <- subset(fold.data, set == "train")
    data.by.neighbors <- list()
    for(neighbors in seq(1, 30, by=2)){
      cat(sprintf("n.folds=%4d validation.fold=%d neighbors=%d\n",
                  n.folds, validation.fold, neighbors))
      pred.label <- 
	knn(only.train[, input.cols, with=FALSE],
            fold.data[, input.cols, with=FALSE],
            only.train$label,
            k=neighbors,
            prob=TRUE)
      prob.winning.class <- attr(pred.label, "prob")
      fold.data$probability <- ifelse(
	pred.label=="1", prob.winning.class, 1-prob.winning.class)
      data.by.neighbors[[paste(neighbors)]] <- 
	data.table(neighbors, fold.data)
    }#for(neighbors
    do.call(rbind, data.by.neighbors)
  }#for(validation.fold
#+END_SRC  

Below, we run the OneFold function in parallel using the doParallel
package. Note that validation folds 1:10 will be used to compute the
validation set error. The validation fold 0 treats all 200
observations as a train set, and will be used for visualizing the
learned decision boundaries of the K-Nearest-Neighbors classifier.

#+BEGIN_SRC R
  library(doParallel)
  registerDoParallel()
  data.all.folds <- foreach(validation.fold=0:10, .combine=rbind) %dopar% {
    one.fold <- OneFold(validation.fold)
    data.table(validation.fold, one.fold)
  }
  data.all.folds[, pred.label := ifelse(0.5 < probability, "1", "0")]
  data.all.folds[, is.error := label != pred.label]
  data.all.folds
#+END_SRC  

The data table of predictions contains almost 3 million observations!
When there are so many data, visualizing all of them at once is not
practical or informative. Instead of visualizing them all at once, we
will compute and plot summary statistics. In the code below we compute
the mean and standard error of the mis-classification error for each
model (over the 10 validation folds).

#+BEGIN_SRC R
  labeled.data <- data.all.folds[!is.na(label),]
  error.stats <- labeled.data[, list(
    error.prop=mean(is.error)
    ), by=.(set, validation.fold, neighbors)]
  validation.error <- error.stats[set=="validation", list(
    mean=mean(error.prop),
    sd=sd(error.prop)/sqrt(.N)
    ), by=.(set, neighbors)]
  validation.error
#+END_SRC

Below we construct data tables for the Bayes error (which we know is
0.21 for the mixture example data), and the train/test error.

#+BEGIN_SRC R
  Bayes.error <- data.table(
    set="Bayes",
    validation.fold=NA,
    neighbors=NA,
    error.prop=0.21)
  Bayes.error
  other.error <- error.stats[validation.fold==0,]
  head(other.error)
#+END_SRC

The code below reproduces the plot of the error curves from the
original Figure. Note that we used the color palette from
dput(RColorBrewer::brewer.pal(Inf, "Set1")).

#+BEGIN_SRC R
  set.colors <-
    c(test="#377EB8", #blue
      validation="#4DAF4A",#green
      Bayes="#984EA3",#purple
      train="#FF7F00")#orange
  errorPlot <- ggplot()+
    geom_hline(aes(yintercept=error.prop, color=set, linetype=set),
               data=Bayes.error)+
    scale_color_manual("error type", values=set.colors, breaks=names(set.colors))+
    scale_linetype_manual("error type", values=c(
      train="solid",
      validation="solid",
      test="solid",
      Bayes="dashed"),
      breaks=names(set.colors))+
    ylab("Misclassification Errors")+
    xlab("Number of Neighbors")+
    geom_linerange(aes(neighbors, ymin=mean-sd, ymax=mean+sd,
                       color=set),
                  data=validation.error)+
    geom_line(aes(neighbors, mean, linetype=set, color=set),
              data=validation.error)+
    geom_line(aes(neighbors, error.prop, group=set, linetype=set, color=set),
              data=other.error)+
    geom_point(aes(neighbors, mean, color=set),
               data=validation.error)+
    geom_point(aes(neighbors, error.prop, color=set),
               data=other.error)
  print(errorPlot)
#+END_SRC  

** Plot of decision boundaries in the input feature space

For the static data visualization of the feature space, we show only
the model with 7 neighbors.

#+BEGIN_SRC R
  show.neighbors <- 7
  show.data <- data.all.folds[validation.fold==0 & neighbors==show.neighbors,]
  show.points <- show.data[set=="train",]
  show.points
#+END_SRC  

Next, we compute the Train, Test, and Bayes mis-classification error
rates which we will show in the bottom left of the feature space plot.

#+BEGIN_SRC R
  text.height <- 0.2
  text.V1.prop <- 0
  text.V2.bottom <- -2
  text.V1.error <- -2.6
  error.text <- rbind(
    Bayes.error,
    other.error[neighbors==show.neighbors,])
  error.text[, V2.top := text.V2.bottom + text.height * (1:.N)]
  error.text[, V2.bottom := V2.top - text.height]
  error.text  
#+END_SRC

We define the following function which we will use to compute the
decision boundaries.

#+BEGIN_SRC R
  getBoundaryDF <- function(prob.vec){
    stopifnot(length(prob.vec) == 6831)
    several.paths <- with(mixture.example, contourLines(
      px1, px2,
      matrix(prob.vec, length(px1), length(px2)),
      levels=0.5))
    contour.list <- list()
    for(path.i in seq_along(several.paths)){
      contour.list[[path.i]] <- with(several.paths[[path.i]], data.table(
	path.i, V1=x, V2=y))
    }
    do.call(rbind, contour.list)
  }
#+END_SRC

We use this function to compute the decision boundaries for the
learned 7-Nearest-Neighbors classifier, and for the optimal Bayes
classifier.

#+BEGIN_SRC R
  boundary.grid <- show.data[set=="grid",]
  boundary.grid[, label := pred.label]
  pred.boundary <- getBoundaryDF(boundary.grid$probability)
  pred.boundary$boundary <- "predicted"
  Bayes.boundary <- getBoundaryDF(mixture.example$prob)
  Bayes.boundary$boundary <- "Bayes"
  Bayes.boundary
#+END_SRC

Below, we consider only the grid points that do not overlap the text
labels.

#+BEGIN_SRC R
  on.text <- function(V1, V2){
    V2 <= max(error.text$V2.top) & V1 <= text.V1.prop
  }
  show.grid <- boundary.grid[!on.text(V1, V2),]
  show.grid
#+END_SRC

The scatterplot below reproduces the 7-Nearest-Neighbors classifier of
the original Figure. 

#+BEGIN_SRC R
  label.colors <-
    c("#E41A1C",
      "0"="#377EB8", "#4DAF4A", "#984EA3",
      "1"="#FF7F00", "#FFFF33", 
      "#A65628", "#F781BF", "#999999")
  scatterPlot <- ggplot()+
    theme_bw()+
    theme(axis.text=element_blank(),
          axis.ticks=element_blank(),
          axis.title=element_blank())+
    ggtitle("7-Nearest Neighbors")+
    scale_color_manual(values=label.colors)+
    scale_linetype_manual(values=c(Bayes="dashed", predicted="solid"))+
    geom_point(aes(V1, V2, color=label),
               size=0.2,
               data=show.grid)+
    geom_path(aes(V1, V2, group=paste(boundary, path.i),
                  linetype=boundary),
              size=1,
              data=pred.boundary)+
    geom_path(aes(V1, V2, group=paste(boundary, path.i),
                  linetype=boundary),
              color=set.colors[["Bayes"]],
              size=1,
              data=Bayes.boundary)+
    geom_point(aes(V1, V2, color=label),
               fill=NA,
               size=3,
               shape=21,
               data=show.points)+
    geom_text(aes(text.V1.error, V2.bottom, label=paste(set, "Error:")),
              data=error.text,
              hjust=0)+
    geom_text(aes(text.V1.prop, V2.bottom, label=sprintf("%.3f", error.prop)),
              data=error.text,
              hjust=1)
  print(scatterPlot)
#+END_SRC  

** Combined plots

Finally, we combine the two ggplots and render them as an animint.

#+BEGIN_SRC R
  viz.static <- list(
    title="Figure 13.4 from Elements of Statistical Learning by Hastie et al",
    error=errorPlot,
    data=scatterPlot
    )
  library(animint)
  structure(viz.static, class="animint")
#+END_SRC  

This data viz does have three interactive legends, but it is static in
the sense that it displays only the model predictions for 7-Nearest
Neighbors.

* Select the number of neighbors using interactivity

In this section we propose an interactive re-design which allows the
user to select K, the number of neighbors in the K-Nearest-Neighbors
classifier.

TODO:SKETCH

** Clickable error curves plot

We begin with a re-design of the error curves plot.

Note the following changes:
- add a selector for the number of neighbors (geom_tallrect).
- change the Bayes decision boundary from a hline with a legend entry,
  to a segment with a text label.
- add a linetype legend to distinguish error rates from the Bayes and
  KNN models.
- change the error bars (geom_linerange) to error bands (geom_ribbon).

The only new data that we need to define are the endpoints of the
segment that we will use to plot the Bayes decision boundary.

#+BEGIN_SRC R
  Bayes.segment <- data.table(
    Bayes.error,
    min.neighbors=1,
    max.neighbors=29,
    set="test",
    error="Bayes")
#+END_SRC

We also add an error variable to the data tables that contain the
prediction error of the K-Nearest-Neighbors models. This error
variable will be used for the linetype legend.

#+BEGIN_SRC R
  validation.error$error <- "KNN"
  other.error$error <- "KNN"
#+END_SRC

We re-define the plot of the error curves below. Note that 
- We use showSelected in geom_text and geom_ribbon, so that they will
  be hidden when the interactive legends are clicked.
- We use clickSelects in geom_tallrect, to select the number of
  neighbors. Clickable geoms should be last (top layer) so that they
  are not obscured by non-clickable geoms (bottom layers).

#+BEGIN_SRC R
  errorPlot <- ggplot()+
    ggtitle("Select number of neighbors")+
    geom_text(aes(min.neighbors, error.prop,
                  color=set, label="Bayes",
                  showSelected=error),
              hjust=1,
              data=Bayes.segment)+
    geom_segment(aes(min.neighbors, error.prop,
                     xend=max.neighbors, yend=error.prop,
                     color=set, linetype=error),
		 data=Bayes.segment)+
    scale_color_manual(values=set.colors, breaks=names(set.colors))+
    scale_fill_manual(values=set.colors)+
    guides(fill="none")+
    scale_linetype_manual("error", values=c(
      KNN="solid",
      Bayes="dashed"))+
    ylab("Misclassification Errors")+
    scale_x_continuous(
      "Number of Neighbors",
      limits=c(-1, 30),
      breaks=c(1, 10, 20, 29))+
    geom_ribbon(aes(neighbors, ymin=mean-sd, ymax=mean+sd,
                    fill=set,
                    showSelected=error,
                    showSelected2=set),
		alpha=0.5,
		data=validation.error)+
    geom_line(aes(neighbors, mean,
                  linetype=error, color=set),
              data=validation.error)+
    geom_line(aes(neighbors, error.prop, group=set,
                  linetype=error, color=set),
              data=other.error)+
    geom_tallrect(aes(xmin=neighbors-1, xmax=neighbors+1,
                      clickSelects=neighbors),
                  alpha=0.5,
                  data=validation.error)
  print(errorPlot)
#+END_SRC  

** Feature space plot that shows the selected number of neighbors 

Next, we focus on a re-design of the feature space plot. In the
previous section we considered only the subset of data from the model
with 7 neighbors. Our re-design includes the following changes:
- We use neighbors as a showSelected variable.
- We add a legend to show which training data points are
  mis-classified.
- We use equal spaced coordinates so that visual distance (pixels) is
  the same as the Euclidean distance in the feature space.

#+BEGIN_SRC R
  show.data <- data.all.folds[validation.fold==0,]
  show.points <- show.data[set=="train",]
  show.points
#+END_SRC  

Below, we compute the predicted decision boundaries separately for
each K-Nearest-Neighbors model.

#+BEGIN_SRC R
  boundary.grid <- show.data[set=="grid",]
  boundary.grid[, label := pred.label]
  show.grid <- boundary.grid[!on.text(V1, V2),]
  pred.boundary <- boundary.grid[, getBoundaryDF(probability), by=neighbors]
  pred.boundary$boundary <- "predicted"
  pred.boundary
#+END_SRC  

Instead of showing the number of neighbors in the plot title, below we
create a geom_text element that will be updated based on the number of
selected neighbors.

#+BEGIN_SRC R
  show.text <- show.grid[, list(
    V1=mean(range(V1)), V2=3.05), by=neighbors]
#+END_SRC

Below we compute the position of the text in the bottom left, which we
will use to display the error rate of the selected model.

#+BEGIN_SRC R
  other.error[, V2.bottom := text.V2.bottom + text.height * 1:2]
#+END_SRC

Below we re-define the Bayes error data without a neighbors column, so
that it appears in each showSelected subset.

#+BEGIN_SRC R
  Bayes.error <- data.table(
    set="Bayes",
    error.prop=0.21)
#+END_SRC

Finally, we re-define the ggplot, using neighbors as a showSelected
variable in the point, path, and text geoms.

#+BEGIN_SRC R
  scatterPlot <- ggplot()+
    theme_bw()+
    xlab("Input feature 1")+
    ylab("Input feature 2")+
    coord_equal()+
    scale_color_manual(values=label.colors)+
    scale_linetype_manual(values=c(Bayes="dashed", predicted="solid"))+
    geom_point(aes(V1, V2, color=label,
                   showSelected=neighbors),
               size=0.2,
               data=show.grid)+
    geom_path(aes(V1, V2, group=paste(boundary, path.i),
                  linetype=boundary, showSelected=neighbors),
              size=1,
              data=pred.boundary)+
    geom_path(aes(V1, V2, group=paste(boundary, path.i),
                  linetype=boundary),
              color=set.colors[["test"]],
              size=1,
              data=Bayes.boundary)+
    geom_point(aes(V1, V2, color=label,
                   fill=is.error,
                   showSelected=neighbors),
               size=3,
               shape=21,
               data=show.points)+
    scale_fill_manual(values=c("TRUE"="black", "FALSE"="transparent"))+
    geom_text(aes(text.V1.error, text.V2.bottom, label=paste(set, "Error:")),
              data=Bayes.error,
              hjust=0)+
    geom_text(aes(text.V1.prop, text.V2.bottom, label=sprintf("%.3f", error.prop)),
              data=Bayes.error,
              hjust=1)+
    geom_text(aes(text.V1.error, V2.bottom, label=paste(set, "Error:"),
                  showSelected=neighbors),
              data=other.error,
              hjust=0)+
    geom_text(aes(text.V1.prop, V2.bottom, label=sprintf("%.3f", error.prop),
                  showSelected=neighbors),
              data=other.error,
              hjust=1)+
    geom_text(aes(V1, V2,
                  showSelected=neighbors,
                  label=paste0(
                    neighbors,
                    " nearest neighbor",
                    ifelse(neighbors==1, "", "s"),
                    " classifier")),
              data=show.text)
#+END_SRC

Before compiling the interactive data viz, we print a static ggplot
with a facet for each value of neighbors.

#+BEGIN_SRC R
  scatterPlot+
    facet_wrap("neighbors")+
    theme(panel.margin=grid::unit(0, "lines"))
#+END_SRC

** Combined interactive data viz

Finally, we combine the two plots in a single data viz with neighbors
as a selector variable.

#+BEGIN_SRC R
  viz.neighbors <- list(
    title="K-Nearest Neighbors in Mixture Example",
    error=errorPlot,
    data=scatterPlot,
    first=list(neighbors=7),
    time=list(variable="neighbors", ms=3000)
    )
  structure(viz.neighbors, class="animint")
#+END_SRC

Note that neighbors is used as a time variable, so animation shows the
predictions of the different models.

* Select the number of cross-validation folds using interactivity

TODO

* Chapter summary and exercises

We showed how to add two interactive features to a data visualization
of predictions of the K-Nearest-Neighbors model. We started with a
static data visualization which only showed predictions of the
7-Nearest-Neighbors model. Then, we created an interactive re-design
which allowed selecting K, the number of neighbors. We did another
re-design which added a plot for selecting the number of
cross-validation folds. 

Exercises:
- So far, the feature space plots only showed model predictions and
  errors for the entire train data set (validation.fold==0). Create a
  re-design which includes a new plot or facet for selecting
  validation.fold, and a facetted feature space plot (one facet for
  train set, one facet for validation set).
